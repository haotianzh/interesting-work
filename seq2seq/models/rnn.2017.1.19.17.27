import tensorflow as tf
from tensorflow.models.rnn import rnn_cell, rnn
from tensorflow.models.rnn import seq2seq
import random
import numpy as np
import copy

class RnnAutoencoder(object):
    def __init__(self, rnn_type, dim_emb, num_units, batch_size, vocab_size, seq_len, 
                 grad_clip, learning_rate, infer):
        self.rnn_type = rnn_type
        self.dim_emb = dim_emb
        self.num_units = num_units
        self.batch_size = batch_size
        self.vocab_size = vocab_size
        self.seq_len = seq_len
        self.grad_clip = grad_clip
        self.lr = learning_rate
        self.TOWER_NAME = 'tower'
       

    def get_inputs(self, infer=False):
        x_in = tf.placeholder(tf.int32, [None, self.seq_len], name="input_x")  # input sequence
        lx_in = tf.placeholder(tf.int32, [None], name="input_lx") # length for input sequence
        y_in = tf.placeholder(tf.int32, [None, 1 if infer else self.seq_len], name = "y_in")  # output sequence
        my_in = tf.placeholder(tf.float32, [None, 1 if infer else self.seq_len]) # mask for output sequence
        return x_in, lx_in, y_in, my_in


    def build_model(self, inputs, infer):
    	x_in, lx_in, y_in, my_in = inputs

        if self.rnn_type == 'rnn':
            cell_fn = rnn_cell.BasicRNNCell
        elif self.rnn_type == 'gru':
            cell_fn = rnn_cell.GRUCell
        elif self.rnn_type == 'lstm':
            cell_fn = rnn_cell.BasicLSTMCell
        else:
            raise Exception('rnn type not supported: {}'.format(rnn_type))

        cell_enc = cell_fn(self.num_units)
        cell_dec = cell_fn(self.num_units)
    
        embedding = tf.get_variable('embedding', [self.vocab_size, self.dim_emb])

        # encoding
        enc_in = tf.nn.embedding_lookup(embedding, x_in)
        enc_in = tf.split(1, self.seq_len, enc_in)
        enc_in = [tf.squeeze(input_, [1]) for input_ in enc_in]

        _, initial_state = rnn.rnn(cell_enc, enc_in, sequence_length=lx_in, dtype='float32', scope='encoder')
        self.initial_state = tf.mul(1.0, initial_state, name = 'initial_state')
        #self.initial_state = initial_state
    
        # decoding
        if infer == False: 
            dec_in = tf.nn.embedding_lookup(embedding, tf.concat(1, [tf.zeros([self.batch_size, 1],
                dtype='int32'), y_in[:, :self.seq_len-1]]))
            dec_in = tf.split(1, self.seq_len, dec_in)
            dec_in = [tf.squeeze(input_, [1]) for input_ in dec_in]
        else:
            dec_in = tf.nn.embedding_lookup(embedding, y_in)
            dec_in = tf.split(1, 1, dec_in)
            dec_in = [tf.squeeze(input_, [1]) for input_ in dec_in]
        
        output, last_state = seq2seq.rnn_decoder(dec_in, initial_state, cell_dec, scope='decoder')
        output = tf.reshape(tf.concat(1, output), [-1, self.num_units])
        self.last_state = tf.mul(1.0, last_state, name = 'last_state')
        #self.last_state = last_state


        # get loss
        #with tf.variable_scope('softmax'):
        softmax_w = tf.get_variable('softmax_w', [self.num_units, self.vocab_size])
        softmax_b = tf.get_variable('softmax_b', [self.vocab_size])

        logits = tf.matmul(output, softmax_w) + softmax_b
        noname_probs = tf.nn.softmax(logits)
        self.probs = tf.mul(1.0, noname_probs, name = 'probs')
        #self.probs = noname_probs

        loss = tf.nn.seq2seq.sequence_loss_by_example([logits],
                                                      [tf.reshape(y_in, [-1])],
                                                      [tf.reshape(my_in, [-1])])

        print "loss shape:", loss.get_shape()
        self.cost = cost = tf.reduce_sum(loss) / tf.to_float(self.batch_size)
        self.loss = cost
        #tvars = tf.trainable_variables()
        #grads = tf.gradients(cost, tvars)
        #if self.grad_clip: grads, _ = tf.clip_by_global_norm(grads, self.grad_clip)
        #optimizer = tf.train.AdamOptimizer(self.lr)
        #self.train_op = optimizer.apply_gradients(zip(grads, tvars))

        return cost

    def select_cand_with_theshold(self, cand_scores, cand_size):
        infer_from_source_num = len(cand_scores)
        max_cand_in_one_source = cand_size

        selected_idx = []
        trans_idx_to_source_id_dict = {}
        cand_flat = cand_scores.flatten()

        top_indices = np.argsort(cand_scores, axis=1)[:, :max_cand_in_one_source]

        bad_generated_sent_lst = []

        tmp_cand_score = []
        for i, row in enumerate(top_indices):
            tmp_row_w_score_lst = []
            for widx in row:
                tmp_row_w_score_lst.append((i, widx, cand_scores[i][widx])) # (wordid, score)
            tmp_cand_score.append(tmp_row_w_score_lst)
        for j in range(max_cand_in_one_source):
            tmp_lst = []
            for i in range(infer_from_source_num):
                tmp_lst.append(tmp_cand_score[i][j])
            tmp_lst.sort(key = lambda x:x[2])
            for local_index_in_column, item in enumerate(tmp_lst):
                if item[2] > 15.0:
                    bad_generated_sent_lst.append(item)
                    continue
                    #break
                trans_idx = item[0]
                word_idx = item[1]
                global_idx = self.vocab_size * trans_idx + word_idx
                selected_idx.append((global_idx, trans_idx, word_idx))
                if len(selected_idx) >= cand_size:
                    return selected_idx, cand_flat, bad_generated_sent_lst
        return selected_idx, cand_flat, bad_generated_sent_lst




    def _is_skip(self, word_pos_in_sent, local_index_in_column, infer_from_source_num):
        if random.random() < 0.1:
            return True
        return False


        return False
    def select_cand_with_skip(self, cand_scores, cand_size):
        infer_from_source_num = len(cand_scores)
        max_cand_in_one_source = cand_size

        selected_idx = []
        trans_idx_to_source_id_dict = {}
        cand_flat = cand_scores.flatten()

        top_indices = np.argsort(cand_scores, axis=1)[:, :max_cand_in_one_source]

        bad_generated_sent_lst = []

        tmp_cand_score = []
        for i, row in enumerate(top_indices):
            tmp_row_w_score_lst = []
            for widx in row:
                tmp_row_w_score_lst.append((i, widx, cand_scores[i][widx])) # (wordid, score)
            tmp_cand_score.append(tmp_row_w_score_lst)
        for j in range(max_cand_in_one_source):
            tmp_lst = []
            for i in range(infer_from_source_num):
                tmp_lst.append(tmp_cand_score[i][j])
            tmp_lst.sort(key = lambda x:x[2])
            for local_index_in_column, item in enumerate(tmp_lst):
                if self._is_skip(j, local_index_in_column, infer_from_source_num):
                    bad_generated_sent_lst.append(item)
                    continue
                    #break
                trans_idx = item[0]
                word_idx = item[1]
                global_idx = self.vocab_size * trans_idx + word_idx
                selected_idx.append((global_idx, trans_idx, word_idx))
                if len(selected_idx) >= cand_size:
                    return selected_idx, cand_flat, bad_generated_sent_lst
        return selected_idx, cand_flat, bad_generated_sent_lst



    def select_cand(self, cand_scores, cand_size):
        infer_from_source_num = len(cand_scores)
        max_cand_in_one_source = cand_size

        selected_idx = []
        trans_idx_to_source_id_dict = {}
        cand_flat = cand_scores.flatten()

        top_indices = np.argsort(cand_scores, axis=1)[:, :max_cand_in_one_source]

        bad_generated_sent_lst = []

        tmp_cand_score = []
        for i, row in enumerate(top_indices):
            tmp_row_w_score_lst = []
            for widx in row:
                tmp_row_w_score_lst.append((i, widx, cand_scores[i][widx])) # (wordid, score)
            tmp_cand_score.append(tmp_row_w_score_lst)
        for j in range(max_cand_in_one_source):
            tmp_lst = []
            for i in range(infer_from_source_num):
                tmp_lst.append(tmp_cand_score[i][j])
            tmp_lst.sort(key = lambda x:x[2])
            for local_index_in_column, item in enumerate(tmp_lst):
                trans_idx = item[0]
                word_idx = item[1]
                global_idx = self.vocab_size * trans_idx + word_idx
                selected_idx.append((global_idx, trans_idx, word_idx))
                if len(selected_idx) >= cand_size:
                    return selected_idx, cand_flat, bad_generated_sent_lst
        return selected_idx, cand_flat, bad_generated_sent_lst


    def _is_good_reply(self, idx2word, black_reply_dict, new_generated_tids):
        if idx2word == None or black_reply_dict == None:
            return True

        r = ""
        for i in new_generated_tids:
            w = idx2word[i].strip().encode("utf8", "ignore")
            if w != "":
                r += w.strip()
        if r in black_reply_dict:
            print "black(common) reply:[", r, "]"
            return False
        return True


    def _debug_string(self, bad_generated_lst, hyp_samples, idx2word):
        for (ti, wi, score) in bad_generated_lst:
            r = ""
            for wid in hyp_samples[ti]:
                w = idx2word[wid].strip().encode("utf8", "ignore")
                if w != "":
                    r += w
            w = idx2word[wi].strip().encode("utf8", "ignore")

            print "bad generated sent:", r ,"->", w, "[" , score,  "]"


    def generate_with_static_beam_size(self, sess, vocab, sym_x, sym_lx, sym_y, x_in, lx_in, beam_size, max_seq_len, idx2word = None, black_reply_dict = None):
        self.count = 0
        state = sess.run(self.initial_state, {sym_x: x_in, sym_lx: lx_in})
        live_k, dead_k = 1, 0
        samples, sample_scores, common_samples, common_sample_scores = [], [], [], []
        hyp_samples, hyp_scores, hyp_states = [[]]*live_k, np.zeros(live_k).astype('float32'), []
            
        yi = np.zeros((1,1), dtype='int32')
        for i in range(max_seq_len):
            prob, state = sess.run([self.probs, self.last_state], {sym_y: yi, self.initial_state: state})
            cand_scores = hyp_scores[:, None] - np.log(prob)
            '''
            cand_flat = cand_scores.flatten()
            ranks_flat = cand_flat.argsort()[: (beam_size - dead_k)]
            trans_idx = ranks_flat / np.int64(self.vocab_size)
            word_idx = ranks_flat % np.int64(self.vocab_size)
            costs = cand_flat[ranks_flat]
            '''

            selected_idx, cand_flat, bad_generated_lst = self.select_cand(cand_scores, beam_size - dead_k)

            self._debug_string(bad_generated_lst, hyp_samples, idx2word)

            print("beam_size:", beam_size, ", dead_k:", dead_k)
            new_hyp_samples, new_hyp_scores, new_hyp_states = [], np.zeros(beam_size - dead_k,
                    dtype='float32'), []
            #for idx, [ti, wi] in enumerate(zip(trans_idx, word_idx)):
            for idx, (global_idx, ti, wi) in enumerate(selected_idx):
                new_hyp_samples.append(hyp_samples[ti] + [wi])
                #new_hyp_scores[idx] = copy.copy(costs[idx])
                new_hyp_scores[idx] = copy.copy(cand_flat[global_idx])
                new_hyp_states.append(copy.copy(state[ti]))

            new_live_k, hyp_samples, hyp_scores, hyp_states = 0, [], [], []
            for idx in range(len(new_hyp_samples)):
                if new_hyp_samples[idx][-1] == 0:
                    if self._is_good_reply(idx2word, black_reply_dict, new_hyp_samples[idx]):
                        samples.append(new_hyp_samples[idx])
                        sample_scores.append(new_hyp_scores[idx])
                        dead_k += 1
                    else:
                        common_samples.append(new_hyp_samples[idx])
                        common_sample_scores.append(new_hyp_scores[idx])
                else:
                    new_live_k += 1
                    hyp_samples.append(new_hyp_samples[idx])
                    hyp_scores.append(new_hyp_scores[idx])
                    hyp_states.append(new_hyp_states[idx])

            hyp_scores = np.asarray(hyp_scores)
            live_k = new_live_k
            if live_k < 1:
                break
            if dead_k >= beam_size:
                break
            yi, state = np.array([[w[-1]] for w in hyp_samples], dtype='int32'), np.array(hyp_states)
            

        if live_k > 0 and False:
            for idx in range(live_k):
                samples.append(hyp_samples[idx])
                sample_scores.append(hyp_scores[idx])
        print("----loop times----", self.count)
        return samples, sample_scores, common_samples, common_sample_scores


    def generate_with_dynamic_beam_size(self, sess, vocab, sym_x, sym_lx, sym_y, x_in, lx_in, beam_size, max_seq_len, idx2word = None, black_reply_dict = None):
        SIZE_DEC_STEP = 2
        tmp_beam_size = beam_size + 10


        self.count = 0
        state = sess.run(self.initial_state, {sym_x: x_in, sym_lx: lx_in})
        live_k, dead_k = 1, 0
        samples, sample_scores, common_samples, common_sample_scores = [], [], [], []
        hyp_samples, hyp_scores, hyp_states = [[]]*live_k, np.zeros(live_k).astype('float32'), []
            
        yi = np.zeros((1,1), dtype='int32')
        for i in range(max_seq_len):
            prob, state = sess.run([self.probs, self.last_state], {sym_y: yi, self.initial_state: state})
            cand_scores = hyp_scores[:, None] - np.log(prob)

            selected_idx, cand_flat, bad_generated_lst = self.select_cand(cand_scores, tmp_beam_size - dead_k)

            self._debug_string(bad_generated_lst, hyp_samples, idx2word)

            print("tmp_beam_size:", tmp_beam_size, ", dead_k:", dead_k)
            new_hyp_samples, new_hyp_scores, new_hyp_states = [], np.zeros(tmp_beam_size - dead_k,
                    dtype='float32'), []
            for idx, (global_idx, ti, wi) in enumerate(selected_idx):
                new_hyp_samples.append(hyp_samples[ti] + [wi])
                new_hyp_scores[idx] = copy.copy(cand_flat[global_idx])
                new_hyp_states.append(copy.copy(state[ti]))

            new_live_k, hyp_samples, hyp_scores, hyp_states = 0, [], [], []
            for idx in range(len(new_hyp_samples)):
                if new_hyp_samples[idx][-1] == 0:
                    if self._is_good_reply(idx2word, black_reply_dict, new_hyp_samples[idx]):
                        samples.append(new_hyp_samples[idx])
                        sample_scores.append(new_hyp_scores[idx])
                        dead_k += 1
                    else:
                        common_samples.append(new_hyp_samples[idx])
                        common_sample_scores.append(new_hyp_scores[idx])
                else:
                    new_live_k += 1
                    hyp_samples.append(new_hyp_samples[idx])
                    hyp_scores.append(new_hyp_scores[idx])
                    hyp_states.append(new_hyp_states[idx])

            hyp_scores = np.asarray(hyp_scores)
            live_k = new_live_k
            if live_k < 1:
                break

            if tmp_beam_size >= beam_size + SIZE_DEC_STEP:
                tmp_beam_size -= SIZE_DEC_STEP
            if dead_k >= beam_size:
                break
            yi, state = np.array([[w[-1]] for w in hyp_samples], dtype='int32'), np.array(hyp_states)
            print "yi shape", yi,shape
            print "state shape", state,shape

        if live_k > 0 and False:
            for idx in range(live_k):
                samples.append(hyp_samples[idx])
                sample_scores.append(hyp_scores[idx])
        print("----loop times----", self.count)
        return samples, sample_scores, common_samples, common_sample_scores
