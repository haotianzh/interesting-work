import tensorflow as tf
from tensorflow.models.rnn import rnn_cell, rnn
from tensorflow.models.rnn import seq2seq

import numpy as np
import copy

class RnnAutoencoder(object):
    def __init__(self, rnn_type, dim_emb, num_units, batch_size, vocab_size, seq_len, 
                 grad_clip, learning_rate, infer):
        self.rnn_type = rnn_type
        self.dim_emb = dim_emb
        self.num_units = num_units
        self.batch_size = batch_size
        self.vocab_size = vocab_size
        self.seq_len = seq_len
        self.grad_clip = grad_clip
        self.lr = learning_rate
        self.TOWER_NAME = 'tower'
       

    def get_inputs(self, infer=False):
        x_in = tf.placeholder(tf.int32, [None, self.seq_len])  # input sequence
        lx_in = tf.placeholder(tf.int32, [None]) # length for input sequence
        y_in = tf.placeholder(tf.int32, [None, 1 if infer else self.seq_len])  # output sequence
        my_in = tf.placeholder(tf.float32, [None, 1 if infer else self.seq_len]) # mask for output sequence
        return x_in, lx_in, y_in, my_in


    def build_model(self, inputs, infer):
    	x_in, lx_in, y_in, my_in = inputs

        if self.rnn_type == 'rnn':
            cell_fn = rnn_cell.BasicRNNCell
        elif self.rnn_type == 'gru':
            cell_fn = rnn_cell.GRUCell
        elif self.rnn_type == 'lstm':
            cell_fn = rnn_cell.BasicLSTMCell
        else:
            raise Exception('rnn type not supported: {}'.format(rnn_type))

        cell_enc = cell_fn(self.num_units)
        cell_dec = cell_fn(self.num_units)
    
        embedding = tf.get_variable('embedding', [self.vocab_size, self.dim_emb])

        # encoding
        enc_in = tf.nn.embedding_lookup(embedding, x_in)
        enc_in = tf.split(1, self.seq_len, enc_in)
        enc_in = [tf.squeeze(input_, [1]) for input_ in enc_in]

        _, initial_state = rnn.rnn(cell_enc, enc_in, sequence_length=lx_in, dtype='float32', scope='encoder')
        self.initial_state = initial_state
    
        # decoding
        if infer == False: 
            dec_in = tf.nn.embedding_lookup(embedding, tf.concat(1, [tf.zeros([self.batch_size, 1],
                dtype='int32'), y_in[:, :self.seq_len-1]]))
            dec_in = tf.split(1, self.seq_len, dec_in)
            dec_in = [tf.squeeze(input_, [1]) for input_ in dec_in]
        else:
            dec_in = tf.nn.embedding_lookup(embedding, y_in)
            dec_in = tf.split(1, 1, dec_in)
            dec_in = [tf.squeeze(input_, [1]) for input_ in dec_in]
        
        output, last_state = seq2seq.rnn_decoder(dec_in, initial_state, cell_dec, scope='decoder')
        output = tf.reshape(tf.concat(1, output), [-1, self.num_units])
        self.last_state = last_state
        
        # get loss
        #with tf.variable_scope('softmax'):
        softmax_w = tf.get_variable('softmax_w', [self.num_units, self.vocab_size])
        softmax_b = tf.get_variable('softmax_b', [self.vocab_size])

        logits = tf.matmul(output, softmax_w) + softmax_b
        print "logits shape:", logits.get_shape()
        self.probs = tf.nn.softmax(logits)
        print "self.probs shape:", self.probs.get_shape()

        loss = tf.nn.seq2seq.sequence_loss_by_example([logits],
                                                      [tf.reshape(y_in, [-1])],
                                                      [tf.reshape(my_in, [-1])])
        #self.loss = loss

        print "loss shape:", loss.get_shape()
        self.cost = cost = tf.reduce_sum(loss) / tf.to_float(self.batch_size)
        self.loss = cost
        #tvars = tf.trainable_variables()
        #grads = tf.gradients(cost, tvars)
        #if self.grad_clip: grads, _ = tf.clip_by_global_norm(grads, self.grad_clip)
        #optimizer = tf.train.AdamOptimizer(self.lr)
        #self.train_op = optimizer.apply_gradients(zip(grads, tvars))

        return cost



    def generate(self, sess, vocab, sym_x, sym_lx, sym_y, x_in, lx_in, beam_size, max_seq_len):
        state = sess.run(self.initial_state, {sym_x: x_in, sym_lx: lx_in})
        live_k, dead_k = 1, 0
        samples, sample_scores = [], []
        hyp_samples, hyp_scores, hyp_states = [[]]*live_k, np.zeros(live_k).astype('float32'), []
            
        yi = np.zeros((1,1), dtype='int32')
        for i in range(max_seq_len):
            prob, state = sess.run([self.probs, self.last_state], {sym_y: yi, self.initial_state: state})
            cand_scores = hyp_scores[:, None] - np.log(prob)

            cand_flat = cand_scores.flatten()
            ranks_flat = cand_flat.argsort()[: (beam_size - dead_k)]

            trans_idx = ranks_flat / np.int64(self.vocab_size)
            word_idx = ranks_flat % np.int64(self.vocab_size)
            costs = cand_flat[ranks_flat]

            new_hyp_samples, new_hyp_scores, new_hyp_states = [], np.zeros(beam_size-dead_k,
                    dtype='float32'), []
            for idx, [ti, wi] in enumerate(zip(trans_idx, word_idx)):
                new_hyp_samples.append(hyp_samples[ti] + [wi])
                new_hyp_scores[idx] = copy.copy(costs[idx])
                new_hyp_states.append(copy.copy(state[ti]))

            new_live_k, hyp_samples, hyp_scores, hyp_states = 0, [], [], []
            for idx in range(len(new_hyp_samples)):
                if new_hyp_samples[idx][-1] == 0:
                    samples.append(new_hyp_samples[idx])
                    sample_scores.append(new_hyp_scores[idx])
                    dead_k += 1
                else:
                    new_live_k += 1
                    hyp_samples.append(new_hyp_samples[idx])
                    hyp_scores.append(new_hyp_scores[idx])
                    hyp_states.append(new_hyp_states[idx])

            hyp_scores = np.asarray(hyp_scores)
            live_k = new_live_k
            if live_k < 1:
                break
            if dead_k >= beam_size:
                break

            yi, state = np.array([[w[-1]] for w in hyp_samples], dtype='int32'), np.array(hyp_states)

        if live_k > 0:
            for idx in range(live_k):
                samples.append(hyp_samples[idx])
                sample_scores.append(hyp_scores[idx])
        
        return samples, sample_scores
